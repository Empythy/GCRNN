{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import pool\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from IPython import display\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(dataName): \n",
    "    train_set = np.loadtxt(dataName + '_TRAIN', delimiter=',')\n",
    "    test_set = np.loadtxt(dataName + '_TEST', delimiter= ',')\n",
    "    trY= (train_set[:, 0]).astype('int32')\n",
    "    teY = ((test_set[:, 0])).astype('int32')\n",
    "    if min(trY)==1:\n",
    "        trY = trY -1\n",
    "        teY = teY -1\n",
    "    if min(trY) ==-1:\n",
    "        trY = (trY +1)/2\n",
    "        teY =(teY +1)/2\n",
    "    trX = (train_set[:, 1:]).astype('float32')\n",
    "    teX = (test_set[:, 1:]).astype('float32')\n",
    "    num_cls = len(set(trY))\n",
    "    #trY = one_hot(trY,num_cls)\n",
    "    #teY = one_hot(teY,num_cls)\n",
    "    length = trX.shape[1]\n",
    "    trX = trX.reshape(-1, 1, length)\n",
    "    teX = teX.reshape(-1, 1, length)\n",
    "    print('dataset ' + dataName + ' class distribution:')\n",
    "    print( Counter(np.append(trY, teY)))\n",
    "    return(trX, trY, teX, teY, length, num_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GCRNN(input_var, length, num_cls, batch_norm = True):\n",
    "    nfeaMaps = [5, 10, 20]\n",
    "    num_node_fc = 100\n",
    "    filter_sizes = [21, 11, 5]\n",
    "    pool_size  = 2\n",
    "    \n",
    "    # input layer\n",
    "    network_input = layers.InputLayer(shape = (None, 1, length), input_var= input_var)\n",
    "\n",
    "    # CONV-RELU-POOL 1\n",
    "    cnn_conv1 = layers.Conv1DLayer(incoming = network_input, num_filters = nfeaMaps[0], filter_size= filter_sizes[0], pad='same')\n",
    "    cnn_pool1 = layers.MaxPool1DLayer(incoming= cnn_conv1, pool_size = pool_size)\n",
    "    if (batch_norm):\n",
    "        cnn_pool1 = layers.BatchNormLayer(incoming=cnn_pool1)\n",
    "\n",
    "    # CONV-RELU-POOL 2\n",
    "    cnn_conv2 = layers.Conv1DLayer(incoming = cnn_pool1, num_filters = nfeaMaps[1], filter_size = filter_sizes[1], pad = 'same')\n",
    "    cnn_pool2 = layers.MaxPool1DLayer(incoming = cnn_conv2, pool_size = pool_size)\n",
    "    if (batch_norm):\n",
    "        cnn_pool2 = layers.BatchNormLayer(incoming=cnn_pool2)\n",
    "\n",
    "    # CONV-RELU-POOL 3 \n",
    "    cnn_conv3 = layers.Conv1DLayer(incoming = cnn_pool2, num_filters = nfeaMaps[2], filter_size = filter_sizes[2], pad = 'same')\n",
    "    cnn_pool3 = layers.MaxPool1DLayer(incoming = cnn_conv3, pool_size = pool_size)\n",
    "    if (batch_norm):\n",
    "        cnn_pool3 = layers.BatchNormLayer(incoming=cnn_pool3)\n",
    "\n",
    "    # RNN input, needs dimension shuffle (time redistributed)\n",
    "    rnn_input = layers.dimshuffle(cnn_pool3, (0,2,1))\n",
    "    \n",
    "    # GRU forward 1\n",
    "    gru_forward1 = layers.GRULayer(incoming = rnn_input, num_units=20, only_return_final=False)\n",
    "    # GRU backward 1\n",
    "    gru_backwards1 = layers.GRULayer(incoming = rnn_input, num_units=20, only_return_final=False, backwards= True)\n",
    "    # GRU merge 1\n",
    "    \n",
    "    gru_merge1 = layers.ConcatLayer([gru_forward1, gru_backwards1], axis= 2)\n",
    "    gru_forward2 = layers.GRULayer(incoming = gru_merge1, num_units=20, only_return_final=False)\n",
    "    # GRU backward 1\n",
    "    gru_backwards2 = layers.GRULayer(incoming = gru_merge1, num_units=20, only_return_final=False, backwards= True)\n",
    "    # GRU merge 1\n",
    "    gru_merge2 = layers.ConcatLayer([gru_forward2, gru_backwards2], axis= 2)\n",
    "    network = layers.DenseLayer(layers.dropout(gru_merge2, p =.5), num_units = num_cls, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## iterate_minibatches function is from the lasagne tutorial \n",
    "## licience: MIT\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_GCRNN( dataName, num_epochs = 100, learning_rate = 0.01, batchsize =50, alpha = 0.8, lasso_l = 0.01, save_model = True):\n",
    "    print('Loading data...')\n",
    "    trX, trY, teX, teY, length, num_cls = load_data(dataName)\n",
    "    X = T.tensor3('X') #Input\n",
    "    Y = T.ivector('Y') #Target\n",
    "\n",
    "    network = GCRNN(X, length, num_cls)\n",
    "\n",
    "    # loss function\n",
    "    prediction  = layers.get_output(network)\n",
    "\n",
    "    ## Loss function setup:\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, Y).mean()\n",
    "    last_layer = layers.get_all_layers(network)[-1]\n",
    "    # L1 panelty\n",
    "    l1_panelty = regularize_layer_params(layers.get_all_layers(network)[-1], l1)*(1-alpha)*lasso_l\n",
    "    # group panelty\n",
    "    group_panelty = T.sum(T.sqrt(T.sum(last_layer.get_params()[0].reshape((-1, 40*num_cls))**2, axis = 1) )) * alpha* lasso_l *T.sqrt(40*num_cls) \n",
    "    loss = loss + l1_panelty + group_panelty \n",
    "\n",
    "    params = layers.get_all_params(network, trainable =True)\n",
    "    updates = lasagne.updates.adagrad(loss, params, learning_rate= learning_rate)\n",
    "\n",
    "    # disabling dropout for testing/validation set\n",
    "    test_prediction = layers.get_output(network, deterministic= True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, Y)\n",
    "    test_loss = test_loss.mean()\n",
    "    # classification accuracy\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis = 1), Y), dtype= theano.config.floatX)\n",
    "\n",
    "    train_fn = theano.function([X, Y], loss, updates = updates)\n",
    "    test_fn = theano.function([X, Y], [test_loss, test_acc])\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    test_loss_epoches = []\n",
    "    test_acc_epoches = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_batches =0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch in iterate_minibatches(trX, trY, batchsize, shuffle = True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        te_loss, te_acc = test_fn(teX, teY)\n",
    "        test_loss_epoches.append(te_loss)\n",
    "        test_acc_epoches.append(te_acc)\n",
    "\n",
    "        # write results for this epoch\n",
    "        print(\"Epoch {} of {}\".format(epoch + 1, num_epochs), end = \"\")\n",
    "        print(\"    training loss:{:.6f}\".format(train_err/train_batches), end=\"\")\n",
    "        print(\"    testing loss:{:.6f}\".format(float(te_loss)),  end=\"\")\n",
    "        print(\"    testing accuracy:{:.2f}%\".format(100*te_acc))\n",
    "\n",
    "\n",
    "    ## visualize the group coefficient to reveal important time series regions\n",
    "    if not os.path.exists('GCRNN_visualization'):\n",
    "        os.makedirs('GCRNN_visualization')\n",
    "\n",
    "    group_coef = T.sqrt(T.sum(last_layer.get_params()[0].reshape((-1, 40*num_cls))**2, axis = 1)).eval()\n",
    "    np.savetxt('GCRNN_visualization/'+ dataName + '_'+ str(alpha)+'_' +str(lasso_l) +  '_group_l2_coef', group_coef, delimiter=',')\n",
    "    fig = plt.figure(figsize=(20, 8), dpi=500)\n",
    "    sample_ind = random.sample(range(len(trY)), len(trY))\n",
    "    color = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'w']\n",
    "\n",
    "    t = range(1,  trX.shape[2]+1)\n",
    "    fig_a = fig.add_subplot(211)\n",
    "    for i in sample_ind:\n",
    "        c = color[trY[i]]\n",
    "        fig_a.plot(t,trX[i][0] , color = c, alpha = 0.4, ls = '-')\n",
    "\n",
    "    fig_b = fig.add_subplot(212)\n",
    "    group_l2 = np.repeat(group_coef, 8)\n",
    "    fig_b.plot(group_l2)\n",
    "    plt.savefig('GCRNN_visualization/'+ dataName + '_'+ str(alpha)+'_' +str(lasso_l)+ '_important_region.png', dpi = 500)\n",
    "\n",
    "    ## save the model\n",
    "    if save_model == True:\n",
    "        if not os.path.exists('GCRNN_models'):\n",
    "            os.makedirs('GCRNN_models')\n",
    "            np.savez('GCRNN_models/'+ dataName + '_'+ str(alpha)+'_' +str(lasso_l)+'_epoch'+ str(num_epochs)+ '.npz', *layers.get_all_param_values(network))\n",
    "    \n",
    "    ## save the testing accuracy\n",
    "    if not os.path.exists('GCRNN_testing_results'):\n",
    "        os.makedirs('GCRNN_testing_results')\n",
    "\n",
    "    np.savetxt('GCRNN_testing_results/'+ dataName + '_'+ str(alpha)+'_' +str(lasso_l)+'_epoch'+ str(num_epochs)+ '_test_acc', test_acc_epoches, delimiter=',')\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    plt.plot(test_acc_epoches, marker='+', color='b')\n",
    "    plt.legend(loc='lower right', prop={'size':10})\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('testing accuracy')\n",
    "    plt.title(dataName)\n",
    "    plt.savefig('GCRNN_testing_results/'+ dataName + '_test_acc.png', dpi = 500)\n",
    "    \n",
    "    return(test_acc_epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset OSULeaf class distribution:\n",
      "Counter({3: 97, 1: 84, 4: 82, 2: 75, 0: 66, 5: 38})\n",
      "Starting training...\n",
      "Epoch 1 of 10    training loss:4.657432    testing loss:1.663544    testing accuracy:34.71%\n",
      "Epoch 2 of 10    training loss:3.121305    testing loss:1.441896    testing accuracy:43.39%\n",
      "Epoch 3 of 10    training loss:2.341209    testing loss:1.426589    testing accuracy:41.32%\n",
      "Epoch 4 of 10    training loss:1.920253    testing loss:1.303332    testing accuracy:44.63%\n",
      "Epoch 5 of 10    training loss:1.718176    testing loss:1.203909    testing accuracy:49.59%\n",
      "Epoch 6 of 10    training loss:1.776080    testing loss:1.236831    testing accuracy:50.00%\n",
      "Epoch 7 of 10    training loss:1.498159    testing loss:1.100256    testing accuracy:53.31%\n",
      "Epoch 8 of 10    training loss:1.421882    testing loss:1.207689    testing accuracy:57.02%\n",
      "Epoch 9 of 10    training loss:1.474748    testing loss:1.084002    testing accuracy:57.02%\n",
      "Epoch 10 of 10    training loss:1.392699    testing loss:1.031189    testing accuracy:57.85%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(0.34710743801652894),\n",
       " array(0.43388429752066116),\n",
       " array(0.4132231404958678),\n",
       " array(0.4462809917355372),\n",
       " array(0.49586776859504134),\n",
       " array(0.5),\n",
       " array(0.5330578512396694),\n",
       " array(0.5702479338842975),\n",
       " array(0.5702479338842975),\n",
       " array(0.5785123966942148)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_GCRNN(dataName='OSULeaf', num_epochs= 10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
